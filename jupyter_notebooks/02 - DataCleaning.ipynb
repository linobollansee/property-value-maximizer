{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **02 - DataCleaning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* Check for missing values in the dataset\n",
        "* Address missing values in the dataset\n",
        "* Implement the data cleaning process\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* outputs/datasets/collection/HousePricesRecords.csv\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* outputs/datasets/cleaned/train_set_cleaned.csv\n",
        "* outputs/datasets/cleaned/test_set_cleaned.csv\n",
        "* outputs/datasets/cleaned/HousePricesCleaned.csv\n",
        "\n",
        "## Additional Comments\n",
        "\n",
        "* The decision to impute or drop variables with missing data should be made carefully because both options can significantly impact the quality and integrity of your model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We are assuming you will store the notebooks in a subfolder, therefore when running the notebook in the editor, you will need to change the working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "We want to make the parent of the current directory the new current directory\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xPk_Ijz3i-"
      },
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Uses pandas library to load dataset into DataFrames df and displays the first 10 rows of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"outputs/datasets/collection/HousePricesRecords.csv\")\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Identifies and extracts the names of columns in the DataFrame df that contain missing (NaN) values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "columns_with_missing_values = df.columns[df.isna().sum() > 0].to_list()\n",
        "columns_with_missing_values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Returns the number of elements in the columns_with_missing_values list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(columns_with_missing_values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Summary only for the columns in columns_with_missing_values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df[columns_with_missing_values].info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If columns_with_missing_values exist, generate a minimal data profile report using the ydata_profiling library. If no columns with missing data are found, print a message indicating there are no columns with missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ydata_profiling import ProfileReport\n",
        "\n",
        "if columns_with_missing_values:\n",
        "    profile = ProfileReport(df=df[columns_with_missing_values], minimal=True)\n",
        "    profile.to_notebook_iframe()\n",
        "else:\n",
        "    print(\"There are no columns with missing data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Analyzes missing data in a DataFrame by calculating the number of missing values and the percentage of missing data for each column. Creates a new DataFrame containing the count of missing values, the percentage of missing data, and the data type for each column. This new DataFrame is sorted by the percentage of missing data in descending order and filtered to include only columns with missing data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def EvaluateMissingData(df):\n",
        "    missing_data_absolute = df.isnull().sum()\n",
        "    missing_data_percentage = round(missing_data_absolute/len(df)*100, 2)\n",
        "    df_missing_data = (pd.DataFrame(\n",
        "                            data={\"RowsWithMissingData\": missing_data_absolute,\n",
        "                                   \"PercentageOfDataset\": missing_data_percentage,\n",
        "                                   \"DataType\": df.dtypes}\n",
        "                                    )\n",
        "                          .sort_values(by=['PercentageOfDataset'], ascending=False)\n",
        "                          .query(\"PercentageOfDataset > 0\")\n",
        "                          )\n",
        "\n",
        "    return df_missing_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "EvaluateMissingData(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualizes the impact of data cleaning by comparing the distribution of specified variables in original and cleaned datasets. It generates bar plots for categorical variables and histograms for numerical ones, to assess the effect of data cleaning visually."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "def DataCleaningEffect(df_original, df_cleaned, variables_applied_with_method):\n",
        "    \"\"\"\n",
        "    Function to visualize data cleaning effect\n",
        "    \"\"\"\n",
        "    flag_count = 1\n",
        "\n",
        "    # Identify categorical variables\n",
        "    categorical_variables = df_original.select_dtypes(exclude=['number']).columns\n",
        "\n",
        "    # Loop through each variable in the list provided\n",
        "    for var in variables_applied_with_method:\n",
        "        print(\"\\n=====================================================================================\")\n",
        "        print(f\"* Distribution Effect Analysis After Data Cleaning Method on variable: {var}\")\n",
        "\n",
        "        if var in categorical_variables:\n",
        "            # For categorical variables, create a bar plot\n",
        "            df1 = pd.DataFrame({\"Type\": \"Original\", \"Value\": df_original[var]})\n",
        "            df2 = pd.DataFrame({\"Type\": \"Cleaned\", \"Value\": df_cleaned[var]})\n",
        "            dfAux = pd.concat([df1, df2], axis=0)\n",
        "            dfAux.reset_index(drop=True, inplace=True)  # Reset the index to avoid duplicates\n",
        "\n",
        "            fig, axes = plt.subplots(figsize=(15, 5))\n",
        "            sns.countplot(data=dfAux, x=\"Value\", hue=\"Type\", palette=['#1f77b4', \"#ff7f0e\"])\n",
        "            axes.set_title(f\"Distribution Plot {flag_count}: {var}\")\n",
        "            plt.xticks(rotation=90)\n",
        "            plt.legend()\n",
        "            plt.show()\n",
        "            print(f\"Displaying bar plot for categorical variable: {var}\")\n",
        "\n",
        "        else:\n",
        "            # For numerical variables, create histograms\n",
        "            fig, axes = plt.subplots(figsize=(10, 5))\n",
        "            sns.histplot(data=df_original, x=var, color=\"#1f77b4\", label='Original', kde=True, element=\"step\", ax=axes)\n",
        "            sns.histplot(data=df_cleaned, x=var, color=\"#ff7f0e\", label='Cleaned', kde=True, element=\"step\", ax=axes)\n",
        "            axes.set_title(f\"Distribution Plot {flag_count}: {var}\")\n",
        "            plt.legend()\n",
        "            plt.show()\n",
        "            print(f\"Displaying histogram for numerical variable: {var}\")\n",
        "\n",
        "        plt.close(fig)\n",
        "        flag_count += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Splits a DataFrame into training and testing sets for machine learning purposes, where TrainSet contains 80% of the data and TestSet contains 20%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "TrainSet, TestSet, _, __ = train_test_split(\n",
        "                                        df,\n",
        "                                        df['SalePrice'],\n",
        "                                        test_size=0.2,\n",
        "                                        random_state=42)\n",
        "\n",
        "print(f\"TrainSet shape: {TrainSet.shape} \\nTestSet shape: {TestSet.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluates and prints information about missing data in the TrainSet DataFrame. The number of variables with missing data is printed, followed by the details of these variables. Additionally, it prints the names of all columns present in the TrainSet DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_missing_data = EvaluateMissingData(TrainSet)\n",
        "print(f\"* There are {df_missing_data.shape[0]} variables with missing data \\n\")\n",
        "print(df_missing_data)\n",
        "\n",
        "print(f\"TrainSet columns: {TrainSet.columns}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Remove columns EnclosedPorch and WoodDeckSF, from the TrainSet DataFrame. They both have an extreme amount of missing data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from feature_engine.selection import DropFeatures\n",
        "\n",
        "variables_to_drop=['EnclosedPorch' , 'WoodDeckSF']\n",
        "imputer = DropFeatures(features_to_drop=variables_to_drop)\n",
        "df_method = imputer.fit_transform(TrainSet)\n",
        "\n",
        "for i in variables_to_drop:\n",
        "    print(i in df_method.columns.to_list())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Applies mean imputation to the numerical variables 'LotFrontage' and 'BedroomAbvGr' in the dataset TrainSet using MeanMedianImputer. The cleaned dataset df_method is passed to the DataCleaningEffect function, which visualizes the impact of imputation by generating histograms for numerical variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from feature_engine.imputation import MeanMedianImputer\n",
        "\n",
        "variables_mean = ['LotFrontage' , 'BedroomAbvGr']\n",
        "imputer = MeanMedianImputer(imputation_method='mean', variables=variables_mean)\n",
        "df_method = imputer.fit_transform(TrainSet)\n",
        "DataCleaningEffect(df_original=TrainSet,\n",
        "                   df_cleaned=df_method,\n",
        "                   variables_applied_with_method=variables_mean)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Applies median imputation to the numerical variables '2ndFlrSF' and 'MasVnrArea' in the dataset TrainSet using MeanMedianImputer. The cleaned dataset df_method is passed to the DataCleaningEffect function, which visualizes the impact of imputation by generating histograms for numerical variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "variables_median = ['2ndFlrSF' , 'MasVnrArea']\n",
        "imputer = MeanMedianImputer(imputation_method='median', variables=variables_median)\n",
        "df_method = imputer.fit_transform(TrainSet)\n",
        "DataCleaningEffect(df_original=TrainSet,\n",
        "                   df_cleaned=df_method,\n",
        "                   variables_applied_with_method=variables_median)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Applies categorical imputation to the categorical variables 'GarageFinish' and 'BsmtFinType1' in the dataset TrainSet using CategoricalImputer. The cleaned dataset df_method is passed to the DataCleaningEffect function, which visualizes the impact of imputation by generating bar plots for categorical variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from feature_engine.imputation import CategoricalImputer\n",
        "\n",
        "variables_categorical = ['GarageFinish' , 'BsmtFinType1']\n",
        "imputer = CategoricalImputer(imputation_method='missing', fill_value='None', variables=variables_categorical)\n",
        "df_method = imputer.fit_transform(TrainSet)\n",
        "DataCleaningEffect(df_original=TrainSet,\n",
        "                   df_cleaned=df_method,\n",
        "                   variables_applied_with_method=variables_categorical)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The cleaning is implemented on Test, Train, and df, followed by a verification to ensure that none contain missing data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "median_imputer = SimpleImputer(strategy='median')\n",
        "\n",
        "numerical_cols_median = ['2ndFlrSF', 'MasVnrArea']\n",
        "\n",
        "TrainSet[numerical_cols_median] = median_imputer.fit_transform(TrainSet[numerical_cols_median])\n",
        "TestSet[numerical_cols_median] = median_imputer.transform(TestSet[numerical_cols_median])\n",
        "df[numerical_cols_median] = median_imputer.transform(df[numerical_cols_median])\n",
        "\n",
        "variables_to_drop = ['EnclosedPorch', 'WoodDeckSF', 'GarageFinish', 'BsmtFinType1', 'BsmtExposure', 'GarageYrBlt']\n",
        "\n",
        "TrainSet = TrainSet.drop(columns=[col for col in variables_to_drop if col in TrainSet.columns], errors='ignore')\n",
        "TestSet = TestSet.drop(columns=[col for col in variables_to_drop if col in TestSet.columns], errors='ignore')\n",
        "df = df.drop(columns=[col for col in variables_to_drop if col in df.columns], errors='ignore')\n",
        "\n",
        "mean_imputer = SimpleImputer(strategy='mean')\n",
        "\n",
        "mean_imputation_cols = ['LotFrontage', 'BedroomAbvGr']\n",
        "\n",
        "TrainSet[mean_imputation_cols] = mean_imputer.fit_transform(TrainSet[mean_imputation_cols])\n",
        "TestSet[mean_imputation_cols] = mean_imputer.transform(TestSet[mean_imputation_cols])\n",
        "df[mean_imputation_cols] = mean_imputer.transform(df[mean_imputation_cols])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prints the count of missing (null) values in three datasets (TrainSet, TestSet, and df) after cleaning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Missing values in TrainSet after cleaning:\")\n",
        "print(TrainSet.isnull().sum())\n",
        "\n",
        "print(\"Missing values in TestSet after cleaning:\")\n",
        "print(TestSet.isnull().sum())\n",
        "\n",
        "print(\"Missing values in df after cleaning:\")\n",
        "print(df.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TrainSet is passed to the EvaluateMissingData function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "EvaluateMissingData(TrainSet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TestSet is passed to the EvaluateMissingData function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "EvaluateMissingData(TestSet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "df is passed to the EvaluateMissingData function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "EvaluateMissingData(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create the directory structure outputs/datasets/cleaned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "try:\n",
        "  os.makedirs(\"outputs/datasets/cleaned\")\n",
        "except Exception as e:\n",
        "  print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save TrainSet as a CSV file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "TrainSet.to_csv(\"outputs/datasets/cleaned/train_set_cleaned.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save TestSet as a CSV file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "TestSet.to_csv(\"outputs/datasets/cleaned/test_set_cleaned.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save df as a CSV file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.to_csv(\"outputs/datasets/cleaned/HousePricesCleaned.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Checks for missing values in the df DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"outputs/datasets/cleaned/HousePricesCleaned.csv\")\n",
        "missing_values = df.isnull().sum()\n",
        "print(missing_values[missing_values > 0])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "3.12.1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
